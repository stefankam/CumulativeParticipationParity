# main_server.py

from flask import Flask, request
import torch
import io
import os
import csv
import time
import copy
import math
from typing import Dict, Tuple
from torchvision import models
from shared_state import topology
import threading
import requests
import socket
from topology_server import TopologyProvider
import shared_state
from availability import extract_availability_vectors

def read_proc_stat() -> Tuple[int, int]:
    with open("/proc/stat", "r") as f:
        cpu_line = f.readline().strip().split()
    values = list(map(int, cpu_line[1:]))
    idle = values[3] + values[4]  # idle + iowait
    total = sum(values)
    return total, idle


def read_meminfo() -> Dict[str, int]:
    info = {}
    with open("/proc/meminfo", "r") as f:
        for line in f:
            key, value = line.split(":", 1)
            info[key.strip()] = int(value.strip().split()[0])
    return info


def read_diskstats() -> Dict[str, int]:
    stats = {}
    with open("/proc/diskstats", "r") as f:
        for line in f:
            parts = line.split()
            if len(parts) < 14:
                continue
            name = parts[2]
            if name.startswith("loop") or name.startswith("ram"):
                continue
            reads = int(parts[3])
            writes = int(parts[7])
            sectors_read = int(parts[5])
            sectors_written = int(parts[9])
            stats[name] = reads + writes + sectors_read + sectors_written
    return stats


def snapshot_system():
    total, idle = read_proc_stat()
    mem = read_meminfo()
    disk = read_diskstats()
    return total, idle, mem, disk


def summarize_system(start, end):
    total0, idle0, mem0, disk0 = start
    total1, idle1, mem1, disk1 = end
    cpu_delta = total1 - total0
    idle_delta = idle1 - idle0
    cpu_pct = 0.0 if cpu_delta == 0 else (1.0 - idle_delta / cpu_delta) * 100
    mem_used_kb = mem1.get("MemTotal", 0) - mem1.get("MemAvailable", 0)
    disk_delta = sum(disk1.values()) - sum(disk0.values())
    return cpu_pct, mem_used_kb, disk_delta


app = Flask(__name__)
current_round = 0

# Global device registry
device_registry = {}
#topology = None  # ‚Üê define globally so update_status() can access it
#current_round = 0  # make current_round global to

# -------------------------------
# 1. REGISTRATION ENDPOINT
# -------------------------------
@app.route("/register", methods=["POST"])
def register():
    data = request.get_json()

    # üß† Use actual sender IP, not what the client claims
#    sender_ip = request.remote_addr

    device_registry[data["device_id"]] = {
        "ip": data["ip"],
        "port": data["port"]
    }

    print(f"üì• Registered {data['device_id']} at {data['ip']}:{data['port']}")

    return "OK", 200

# Distributed Hash Table
class DHT:
    def __init__(self, size=100):
        self.table = {}
        self.size = size

@app.route("/ready", methods=["GET"])
def ready():
    if shared_state.topology:
        return "ready", 200
    return "not_ready", 503

def initialize_topology(device_file="devices.txt", num_clients=3):
    print("‚è≥ Waiting for clients to register...")
    while len(device_registry) < num_clients:
        print(f"üïí Registered devices: {len(device_registry)} / {num_clients}")
        time.sleep(2)

    print("‚úÖ All clients registered. Initializing topology.")

#    print("üìÑ Loading devices from file...")

#    device_registry = {}
#    with open(device_file, "r") as f:
#        for i, line in enumerate(f):
#            if i >= num_clients:
#                break
#            if not line.strip():
#                continue
#            device_id, ip, port = line.strip().split()
#            device_registry[device_id] = {
#                "ip": ip,
#                "port": int(port)
#            }

#    print(f"‚úÖ Loaded {len(device_registry)} devices.")

    device_ids = list(device_registry.keys())

    shared_state.topology = TopologyProvider(
        device_names=device_ids,
        num_epochs=1,
        link_latency=20, 
        link_loss=5,
        model_name='resnet',
        device_registry=device_registry 
    )
    shared_state.topology.dht = DHT(size=100)  # Initialize the DHT
    for device_id in device_ids:
        shared_state.topology.dht.table[device_id] = {
          "latency": None,
          "packet_loss": None,
          "last_seen": None,
          "availability": None,
          "freshness": None,
          "correlation": None
        }
    print("‚úÖ Topology initialized.")
    wait_for_latency_data()



@app.route("/status_update", methods=["POST"])
def update_status():
    global current_round
    data = request.get_json()
    node = data["device_id"]

    if shared_state.topology is None:
        print("‚ö†Ô∏è Topology not initialized yet. Ignoring status update.")
        return "ERROR: Topology not initialized", 503

    if node not in shared_state.topology.dht.table:
        print(f"‚ö†Ô∏è Node {node} not found in DHT.")
        return "ERROR: Node not found", 404

    shared_state.topology.dht.table[node]["latency"] = data["latency"]
    shared_state.topology.dht.table[node]["packet_loss"] = data["packet_loss"]
    shared_state.topology.dht.table[node]["last_seen"] = time.time()
    shared_state.topology.dht.table[node]["availability"] = data["availability"]
    shared_state.topology.dht.table[node]["freshness"] = shared_state.topology.get_freshness(node, current_round)
    shared_state.topology.dht.table[node]["correlation"] = shared_state.topology.failure_correlation.get(node, {})
    print(f"üì∂ Updated status for {node}: latency={data['latency']}, loss={data['packet_loss']}")


    return "OK", 200

# -------------------------------
# 2. HEALTH CHECK (optional)
# -------------------------------
@app.route("/health", methods=["GET"])
def health():
    return "OK", 200

# -------------------------------
# 3. FEDERATED COORDINATOR
# -------------------------------

import torch.nn as nn
from torchvision import models

def init_resnet(train_last_n_blocks=1):
    base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
    base_model.fc = nn.Linear(base_model.fc.in_features, 10)

    # Freeze everything first
    for param in base_model.parameters():
        param.requires_grad = False

    # Unfreeze last N blocks + FC
    if train_last_n_blocks >= 1:
        for param in base_model.layer4.parameters():
            param.requires_grad = True
    if train_last_n_blocks >= 2:
        for param in base_model.layer3.parameters():
            param.requires_grad = True
    if train_last_n_blocks >= 3:
        for param in base_model.layer2.parameters():
            param.requires_grad = True

    # Always unfreeze fc
    for param in base_model.fc.parameters():
        param.requires_grad = True

    return base_model



def run_federated_training():
    global current_round
    if shared_state.topology is None:
        raise RuntimeError("‚ùå Topology has not been initialized.")

    label_map = shared_state.topology.label_map

    # Load availability vectors
    availability_vectors = extract_availability_vectors("traces/traces.txt")

    awpsp_accuracy_log = []
    psp_accuracy_log = [] 
    awpsp_instant_fairness_log = []
    psp_instant_fairness_log = []
    awpsp_cumul_fairness_log = []
    psp_cumul_fairness_log = []  
    corr_failure_log = []
    awpsp_covered_labels_log = []
    psp_covered_labels_log = []
    selected_awpsp_log = []
    selected_psp_log = []
    awpsp_avg_score_log = []
    psp_avg_score_log = []
    awpsp_labels_log =[]
    psp_labels_log =[]
    awpsp_KL_log =[]
    psp_KL_log =[]
    awpsp_unseen_log =[]
    psp_unseen_log =[]
    awpsp_gini_log =[]
    psp_gini_log =[]
    accuracy_log = []
    var_u_log = []
    surrogate_log = []

    # ---------------- Initialize models ----------------
#    base_model = models.resnet18(weights=None)
    base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
    base_model.fc = torch.nn.Linear(base_model.fc.in_features, 10)
#    base_model = init_resnet(train_last_n_blocks=2)  # train layer3 + layer4 + fc

    # Two independent weight states
    current_weights_awpsp = base_model.state_dict()
    current_weights_psp = copy.deepcopy(current_weights_awpsp)


    def compute_final_metrics(model=None, round_index=None):
        if model is None:
            model = base_model
        if round_index is None:
            round_index = current_round
        nodes = list(shared_state.topology.dht.table.keys())
        per_client_acc = shared_state.topology.evaluate_per_client_accuracy(model, nodes)
        acc_values = [val for val in per_client_acc.values() if val is not None]
        if not acc_values:
            return None

        avg_acc = sum(acc_values) / len(acc_values)
        acc_variance = sum((val - avg_acc) ** 2 for val in acc_values) / len(acc_values)
        acc_squared_sum = sum(val ** 2 for val in acc_values)
        jain_acc = (sum(acc_values) ** 2) / (len(acc_values) * acc_squared_sum) if acc_squared_sum else 0.0

        u_tilde_values = []
        u_tilde_with_surrogate = []
        for node in nodes:
            if shared_state.topology.total_rounds_elapsed > 0:
                pi_k = shared_state.topology.availability_counts[node] / shared_state.topology.total_rounds_elapsed
                if pi_k > 0:
                    u_k = shared_state.topology.utility_log[node]
                    u_tilde_values.append(u_k / pi_k)
                    surrogate_k = shared_state.topology.surrogate_contributions.get(node, 0.0)
                    u_tilde_with_surrogate.append((u_k + surrogate_k) / pi_k)

        def compute_utility_metrics(values):
            if not values:
                return None, None
            mean_u = sum(values) / len(values)
            std_u = math.sqrt(sum((val - mean_u) ** 2 for val in values) / len(values))
            utility_cv = (std_u / mean_u) if mean_u != 0 else 0.0
            squared_sum = sum(val ** 2 for val in values)
            jain_utility = (sum(values) ** 2) / (len(values) * squared_sum) if squared_sum else 0.0
            return utility_cv, jain_utility

        selected_counts = [len(shared_state.topology.participation_log.get(node, [])) for node in nodes]
        sel_gap = max(selected_counts) - min(selected_counts) if selected_counts else 0.0
        if selected_counts and sum(selected_counts) > 0:
            diffs = 0.0
            for i in selected_counts:
                for j in selected_counts:
                    diffs += abs(i - j)
            gini = diffs / (2 * len(selected_counts) * sum(selected_counts))
        else:
            gini = 0.0

        utility_cv_no, jain_utility_no = compute_utility_metrics(u_tilde_values)
        utility_cv_with, jain_utility_with = compute_utility_metrics(u_tilde_with_surrogate)

        return {
            "Round": round_index + 1,
            "Avg Acc (No Surrogate)": avg_acc,
            "Jain (Acc) (No Surrogate)": jain_acc,
            "Utility CV (No Surrogate)": utility_cv_no,
            "Jain (Utility) (No Surrogate)": jain_utility_no,
            "Sel. Gap (No Surrogate)": sel_gap,
            "Gini (No Surrogate)": gini,
            "Avg Acc (With Surrogate)": avg_acc,
            "Jain (Acc) (With Surrogate)": jain_acc,
            "Utility CV (With Surrogate)": utility_cv_with,
            "Jain (Utility) (With Surrogate)": jain_utility_with,
            "Sel. Gap (With Surrogate)": sel_gap,
            "Gini (With Surrogate)": gini,
            "Acc Variance": acc_variance,
        }

    num_rounds = 50
    for current_round in range(num_rounds):
        print(f"\nüåê Federated Round {current_round + 1}")

        round_start = time.perf_counter()
        sys_start = snapshot_system()

        # Detect correlated failures
        correlated_failures = shared_state.topology.get_correlated_failure(
            current_round, availability_vectors, corr_threshold=0.35, num_neighbors=4
        )
        num_corr_failed = sum(1 for _, failed_neighbors in correlated_failures if failed_neighbors)
        corr_failure_log.append((current_round, num_corr_failed))
        print(f"üå©Ô∏è Correlated failure count: {num_corr_failed}")
        selected, var_u, total_bias_bound = shared_state.topology.select_fair_nodes(
            base_model,
            current_round,
            correlated_failures,
            num_clients=5,
            corr_threshold=0.35,
            label_map=label_map,
            lambda_=0.5,
            epsilon=1e-5,
        )
        selection_end = time.perf_counter()
        weights_fair = shared_state.topology.run_federated_round(selected, current_weights_awpsp, base_model)
        if weights_fair is not None:
           base_model.load_state_dict(weights_fair)
           current_weights_awpsp = weights_fair
           accuracy = shared_state.topology.evaluate_global_model(base_model, use_selected_nodes=False)
           accuracy_log.append((current_round, accuracy))
           var_u_log.append((current_round, var_u))
           surrogate_log.append((current_round, total_bias_bound))
           print(f"üîÅ Round {current_round + 1}: Fair-Select Acc = {accuracy:.2f}%")
        else:
           print("‚ö†Ô∏è No updates received from clients. Skipping model update this round.")
        fair_end = time.perf_counter()

        # ---------------- AW-PSP branch ----------------
        # Node selection based on AW-PSP

#        awpsp_model = models.resnet18(weights=None)
        awpsp_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        awpsp_model.fc = torch.nn.Linear(awpsp_model.fc.in_features, 10)
        awpsp_model.load_state_dict(current_weights_awpsp)

        selected_awpsp, awpsp_instant_var, awpsp_cumul_var, awpsp_covered_labels, awpsp_avg_score, awpsp_labels, awpsp_KL, awpsp_unseen, awpsp_gini = \
            shared_state.topology.prioritize_available_nodes(
                awpsp_model, current_round, correlated_failures, num_clients=5, label_map=label_map
            )
        awpsp_covered_labels_log.append((current_round, len(awpsp_covered_labels)))
        selected_awpsp_log.append((current_round, selected_awpsp))

        weights_awpsp = shared_state.topology.run_federated_round(selected_awpsp, current_weights_awpsp, awpsp_model)
        if weights_awpsp is not None:
           current_weights_awpsp = weights_awpsp
           awpsp_model.load_state_dict(current_weights_awpsp)
           accuracy_awpsp = shared_state.topology.evaluate_global_model(awpsp_model, selected_nodes=selected_awpsp, use_selected_nodes=False)
           awpsp_accuracy_log.append((current_round, accuracy_awpsp))
           awpsp_instant_fairness_log.append((current_round, awpsp_instant_var))
           awpsp_cumul_fairness_log.append((current_round, awpsp_cumul_var))
           print(f"üîÅ Round {current_round + 1}: AW-PSP Acc = {accuracy_awpsp:.2f}%")
           awpsp_avg_score_log.append((current_round, awpsp_avg_score))
           awpsp_labels_log.append((current_round, awpsp_labels))
           awpsp_KL_log.append((current_round, awpsp_KL))
           awpsp_unseen_log.append((current_round, awpsp_unseen))
           awpsp_gini_log.append((current_round, awpsp_gini))
        else:
           print("‚ö†Ô∏è No updates received from clients. Skipping model update this round.")
        awpsp_end = time.perf_counter()

        # ---------------- PSP branch ----------------
        # Use a fresh model copy so AW-PSP doesn‚Äôt pollute PSP results
        #psp_model = models.resnet18(weights=None)
        psp_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        psp_model.fc = torch.nn.Linear(psp_model.fc.in_features, 10)
        psp_model.load_state_dict(current_weights_psp)

        selected_psp, psp_instant_var, psp_cumul_var, psp_covered_labels, psp_avg_score, psp_labels, psp_KL, psp_unseen, psp_gini = \
            shared_state.topology.psp_random_selection(
                psp_model, correlated_failures, num_clients=5, label_map=label_map
            )
        psp_covered_labels_log.append((current_round, len(psp_covered_labels)))
        selected_psp_log.append((current_round, selected_psp))

        weights_psp = shared_state.topology.run_federated_round(selected_psp, current_weights_psp,psp_model)
        if weights_psp is not None:
           current_weights_psp = weights_psp
           psp_model.load_state_dict(current_weights_psp)
           accuracy_psp = shared_state.topology.evaluate_global_model(psp_model, selected_nodes=selected_psp, use_selected_nodes=False)
           psp_accuracy_log.append((current_round, accuracy_psp))
           psp_instant_fairness_log.append((current_round, psp_instant_var))
           psp_cumul_fairness_log.append((current_round, psp_cumul_var))
           print(f"üîÅ Round {current_round + 1}: PSP Acc = {accuracy_psp:.2f}%")
           psp_avg_score_log.append((current_round, psp_avg_score))
           psp_labels_log.append((current_round, psp_labels))
           psp_KL_log.append((current_round, psp_KL))
           psp_unseen_log.append((current_round, psp_unseen))
           psp_gini_log.append((current_round, psp_gini))
        else:
           print("‚ö†Ô∏è No updates received from clients. Skipping model update this round.")

        psp_end = time.perf_counter()

        # Save logs to CSV
        with open("metrics_log.csv", "w") as f:
          writer = csv.writer(f)
          writer.writerow(["Round", "Select_Fair_Accuracy", "Select_Fair_variance", "Select_Fair_Surrogate", "AWPSP_Accuracy", "AWPSP_instant_fairness", "AWPSP_cumul_fairness","CorrelatedFailureCount", "AWPSP_CoveredLabelsCount", "PSP_Accuracy", "PSP_instant_fairness", "PSP_cumul_fairness", "PSP_CoveredLAbelsCount", "selected_awpsp", "selected_psp", "AWPSP Avg Score", "PSP Avg Score", "AWPSP labels", "PSP labels", "AWPSP KL", "PSP KL", "AWPSP unseen", "PSP unseen", "AWPSP gini", "PSP gini"])
          for i in range(num_rounds):
            print(i, accuracy_log[i][1] if i < len(accuracy_log) else None, var_u_log[i][1] if i < len(var_u_log) else None, surrogate_log[i][1] if i < len(surrogate_log) else None, awpsp_accuracy_log[i][1] if i < len(awpsp_accuracy_log) else None, awpsp_instant_fairness_log[i][1] if i < len(awpsp_instant_fairness_log) else None,  awpsp_cumul_fairness_log[i][1] if i < len(awpsp_cumul_fairness_log) else None, corr_failure_log[i][1] if i < len(corr_failure_log) else None, awpsp_covered_labels_log[i][1] if i < len(awpsp_covered_labels_log) else None, psp_accuracy_log[i][1] if i < len(psp_accuracy_log) else None, psp_instant_fairness_log[i][1] if i < len(psp_instant_fairness_log) else None, psp_cumul_fairness_log[i][1] if i < len(psp_cumul_fairness_log) else None, psp_covered_labels_log[i][1] if i < len(psp_covered_labels_log) else None, selected_awpsp_log[i][1] if i < len(selected_awpsp_log) else None, selected_psp_log[i][1] if i < len(selected_psp_log) else None, awpsp_avg_score_log[i][1] if i < len(awpsp_avg_score_log) else None, psp_avg_score_log[i][1] if i < len(psp_avg_score_log) else None, awpsp_labels_log[i][1] if i < len(awpsp_labels_log) else None, psp_labels_log[i][1] if i < len(psp_labels_log) else None, awpsp_KL_log[i][1] if i < len(awpsp_KL_log) else None, psp_KL_log[i][1] if i < len(psp_KL_log) else None, awpsp_unseen_log[i][1] if i < len(awpsp_unseen_log) else None, psp_unseen_log[i][1] if i < len(psp_unseen_log) else None, awpsp_gini_log[i][1] if i < len(awpsp_gini_log) else None, psp_gini_log[i][1] if i < len(psp_gini_log) else None)
            writer.writerow([
              i,
              accuracy_log[i][1] if i < len(accuracy_log) else None,
              var_u_log[i][1] if i < len(var_u_log) else None,
              surrogate_log[i][1] if i < len(surrogate_log) else None,
              awpsp_accuracy_log[i][1] if i < len(awpsp_accuracy_log) else None,
              awpsp_instant_fairness_log[i][1] if i < len(awpsp_instant_fairness_log) else None,
              awpsp_cumul_fairness_log[i][1] if i < len(awpsp_cumul_fairness_log) else None,
              corr_failure_log[i][1] if i < len(corr_failure_log) else None,
              awpsp_covered_labels_log[i][1] if i < len(awpsp_covered_labels_log) else None,
              psp_accuracy_log[i][1] if i < len(psp_accuracy_log) else None,
              psp_instant_fairness_log[i][1] if i < len(psp_instant_fairness_log) else None,
              psp_cumul_fairness_log[i][1] if i < len(psp_cumul_fairness_log) else None,
              psp_covered_labels_log[i][1] if i < len(psp_covered_labels_log) else None,
              selected_awpsp_log[i][1] if i < len(selected_awpsp_log) else None,
              selected_psp_log[i][1] if i < len(selected_psp_log) else None,
              awpsp_avg_score_log[i][1] if i < len(awpsp_avg_score_log) else None,
              psp_avg_score_log[i][1] if i < len(psp_avg_score_log) else None,
              awpsp_labels_log[i][1] if i < len(awpsp_labels_log) else None,
              psp_labels_log[i][1] if i < len(psp_labels_log) else None,
              awpsp_KL_log[i][1] if i < len(awpsp_KL_log) else None,
              psp_KL_log[i][1] if i < len(psp_KL_log) else None,
              awpsp_unseen_log[i][1] if i < len(awpsp_unseen_log) else None,
              psp_unseen_log[i][1] if i < len(psp_unseen_log) else None,
              awpsp_gini_log[i][1] if i < len(awpsp_gini_log) else None,
              psp_gini_log[i][1] if i < len(psp_gini_log) else None,
        ])

        summary = compute_final_metrics(base_model, current_round)
        if summary:
            print("Final metrics summary:")
            for key, value in summary.items():
                print(f"{key}: {value}")
            write_header = not os.path.exists("final_metrics.csv")
            with open("final_metrics.csv", "a") as f:
                writer = csv.writer(f)
                if write_header:
                    writer.writerow(list(summary.keys()))
                writer.writerow(list(summary.values()))

        sys_end = snapshot_system()
        cpu_pct, mem_used_kb, disk_delta = summarize_system(sys_start, sys_end)
        print(
            "üìä Round timing: selection={:.2f}s fair={:.2f}s awpsp={:.2f}s psp={:.2f}s total={:.2f}s".format(
                selection_end - round_start,
                fair_end - selection_end,
                awpsp_end - fair_end,
                psp_end - awpsp_end,
                psp_end - round_start,
            )
        )
        print(
            "üßÆ System usage: CPU~{:.1f}% MemUsed~{:.1f}MB DiskDelta~{}"
            .format(cpu_pct, mem_used_kb / 1024.0, disk_delta)
        )


def wait_for_latency_data(num_clients=3):
    print("‚è≥ Waiting for latency updates from clients...")
    while True:
        ready = 0
        for node, metadata in shared_state.topology.dht.table.items():
            if metadata.get("latency") is not None and metadata.get("packet_loss") is not None:
                ready += 1
        print(f"‚úÖ Clients with latency info: {ready} / {num_clients}")
        if ready >= num_clients:
            break
        time.sleep(2)

    print("üöÄ Sufficient clients reported latency. Starting training.")
    run_federated_training()



if __name__ == "__main__":
    print("üöÄ Starting main server HTTP API...")


    # Step 1: Start the server in a separate thread
    threading.Thread(target=lambda: app.run(host="0.0.0.0", port=8080), daemon=True).start()

    # Step 2: Start topology initialization in the background
    threading.Thread(target=initialize_topology).start()

    # Step 3: Wait for latency info from clients, then start training
    #threading.Thread(target=wait_for_latency_data).start()
